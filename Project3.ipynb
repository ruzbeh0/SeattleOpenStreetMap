{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seattle, WA - OpenStreetMap Data Wrangling with MongoDB\n",
    "\n",
    "This project will wrangle and analyze data from the OpenStreetMap project in the Seattle, WA area.\n",
    "\n",
    "The data was downloaded from this link: https://mapzen.com/data/metro-extracts/metro/seattle_washington/\n",
    "\n",
    "The code below imports libraries necessary for this project, the OSM file name and regex expressions that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "OSMFILE = \"seattle_washington.osm\"\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this project, the street names will be standardized. In this dataset, a few incosistencies with street names and abbreviations were found. For example, there are several streets which include a compass direction in their name (Northeast, Northwest, Southeast, Southwest, etc). Different conventions were found, some streets were abbreviated as N.E. while others just used NE and some included the full word. The mapping below will be used to standardize those names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"NE\": \"Northeast\",\n",
    "            \"N.E.\": \"Northeast\",\n",
    "            \"NW\": \"Northwest\",\n",
    "            \"N.W.\": \"Northwest\",\n",
    "            \"S\": \"South\",\n",
    "            \"SE\": \"Southeast\",\n",
    "            \"S.E.\": \"Southeast\",\n",
    "            \"SW\": \"Southwest\",\n",
    "            \"S.W.\": \"Southwest\",\n",
    "            \"W\" : \"West\",\n",
    "            \"E\" : \"East\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is_street_name function will verify if a particular XML element is a street name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update_name function will use the mapping variable above to standardize street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        for key, value in mapping.iteritems():\n",
    "            if street_type == key:\n",
    "                name = name.replace(key,value)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape_element function receives an XML element and converts it to JSON format to later be inserted in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        lat = str(element.get(\"lat\"))\n",
    "        lon = str(element.get(\"lon\"))\n",
    "        try:\n",
    "            node[\"pos\"] = [float(lat),float(lon)]\n",
    "        except:\n",
    "            pass\n",
    "        created = {}\n",
    "        created[\"changeset\"] = element.get(\"changeset\")\n",
    "        created[\"user\"] = element.get(\"user\")\n",
    "        created[\"version\"] = element.get(\"version\")\n",
    "        created[\"uid\"] = element.get(\"uid\")\n",
    "        created[\"timestamp\"] = element.get(\"timestamp\")\n",
    "        node[\"created\"] = created\n",
    "        node[\"visible\"] = element.get(\"visible\")\n",
    "        node[\"type\"] = element.tag\n",
    "        node[\"id\"] = element.get(\"id\")\n",
    "        \n",
    "        address = {}\n",
    "        for subelement in element.iter(\"tag\"):\n",
    "            k_element = subelement.get(\"k\")\n",
    "            v_element = subelement.get(\"v\")\n",
    "            if not problemchars.match(k_element):\n",
    "                if k_element.startswith(\"addr:\"):\n",
    "                    if is_street_name(subelement):\n",
    "                        v_element = update_name(v_element,mapping)\n",
    "                    k_elements = k_element.split(\":\")\n",
    "                    if(len(k_elements) < 3):\n",
    "                        address[k_elements[1]] = v_element\n",
    "                else:\n",
    "                    node[k_element] = v_element\n",
    "        if(bool(address)):\n",
    "            node[\"address\"] = address\n",
    "            \n",
    "        if element.tag == \"way\":\n",
    "            node_refs = []\n",
    "            for subelement in element.iter(\"nd\"):\n",
    "                node_refs.append(subelement.get(\"ref\"))\n",
    "            node[\"node_refs\"] = node_refs\n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process_map function receives the OSM input file and inserts it to the database. Due to the OSM file size for the Seattle area (1.61 GB) this function will insert each element at a time in the database to prevent jupyter notebook from reaching its memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_map(file_in, db_table):\n",
    "    data = []\n",
    "    i = 0\n",
    "    for _, element in ET.iterparse(file_in):\n",
    "        el = shape_element(element)\n",
    "        if el != None:\n",
    "            data.append(el)\n",
    "            i = i + 1\n",
    "            #Insert every 10,000 records to the database\n",
    "            if i == 10000:\n",
    "                db_table.insert_many(data)\n",
    "                #Empty data list and restart count\n",
    "                data[:] = []\n",
    "                i = 0\n",
    "    #Insert rest of the data list to the database\n",
    "    db_table.insert_many(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_db function returns the MongoDB database instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_db():\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client.seattle\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will use the functions declared above and load the OSM file and save it to the MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = get_db()\n",
    "\n",
    "db.seattle_data.drop()\n",
    "\n",
    "process_map(OSMFILE, db.seattle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query below we will count the number of documents that we imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7940891"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.seattle_data.find().count()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will count the number of nodes and ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7233040"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.seattle_data.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707752"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.seattle_data.find({\"type\":\"way\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also investigate some other characteristics of the dataset, such as the top 10 types of parking structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': None, u'count': 6037}\n",
      "{u'_id': u'surface', u'count': 3522}\n",
      "{u'_id': u'multi-storey', u'count': 149}\n",
      "{u'_id': u'underground', u'count': 42}\n",
      "{u'_id': u'rooftop', u'count': 17}\n",
      "{u'_id': u'carports', u'count': 10}\n",
      "{u'_id': u'garage', u'count': 6}\n",
      "{u'_id': u'park_and_ride', u'count': 5}\n",
      "{u'_id': u'lane', u'count': 1}\n",
      "{u'_id': u'Asphalt', u'count': 1}\n"
     ]
    }
   ],
   "source": [
    "match = {\"$match\":{\"amenity\":{\"$eq\":\"parking\"}}}\n",
    "group = {\"$group\":{\"_id\":\"$parking\", \"count\":{\"$sum\":1}}}\n",
    "sort = {\"$sort\":{\"count\": -1}}\n",
    "limit = {\"$limit\" : 10}\n",
    "result = db.seattle_data.aggregate([match, group, sort, limit])\n",
    "\n",
    "for r in result:\n",
    "    pprint.pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the top 10 cities with the highest number of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'Seattle', u'count': 203295}\n",
      "{u'_id': u'Kirkland', u'count': 42285}\n",
      "{u'_id': u'Saanich', u'count': 11625}\n",
      "{u'_id': u'Mount Vernon', u'count': 11524}\n",
      "{u'_id': u'Langford', u'count': 2810}\n",
      "{u'_id': u'Oak Bay', u'count': 2298}\n",
      "{u'_id': u'Colwood', u'count': 1985}\n",
      "{u'_id': u'Sooke', u'count': 1600}\n",
      "{u'_id': u'Esquimalt', u'count': 1495}\n",
      "{u'_id': u'View Royal', u'count': 1012}\n"
     ]
    }
   ],
   "source": [
    "match = {\"$match\":{\"address.city\":{\"$exists\":1}}}\n",
    "group = {\"$group\":{\"_id\":\"$address.city\", \"count\":{\"$sum\":1}}}\n",
    "sort = {\"$sort\":{\"count\": -1}}\n",
    "limit = {\"$limit\" : 10}\n",
    "result = db.seattle_data.aggregate([match, group, sort, limit])\n",
    "\n",
    "for r in result:\n",
    "    pprint.pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A small portion of this dataset is located in Canada. In the next steps we are going to investigate if the records correctly reference the country in which they are located. We will start by grouping the country field and seeing which values are being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'US', u'count': 2466}\n",
      "{u'_id': u'CA', u'count': 50}\n",
      "{u'_id': None, u'count': 7938375}\n"
     ]
    }
   ],
   "source": [
    "group = {\"$group\":{\"_id\":\"$address.country\", \"count\":{\"$sum\":1}}}\n",
    "limit = {\"$limit\": 100}\n",
    "result = db.seattle_data.aggregate([group, limit])\n",
    "\n",
    "for r in result:\n",
    "    pprint.pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query above shows that the majority of the records do not include country information. As an alternative we could investigate records which have a province or state record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'British Columbia', u'count': 5}\n",
      "{u'_id': u'BC', u'count': 136}\n",
      "{u'_id': None, u'count': 7940750}\n"
     ]
    }
   ],
   "source": [
    "group = {\"$group\":{\"_id\":\"$address.province\", \"count\":{\"$sum\":1}}}\n",
    "limit = {\"$limit\": 100}\n",
    "result = db.seattle_data.aggregate([group, limit])\n",
    "\n",
    "for r in result:\n",
    "    pprint.pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query results above shows that there are a few records without a country which are located in British Columbia. We will set the country as Canada for those records an standardize all of them to use the same convention for the province name. After that we will set all of those records to have 'CA' as the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x57d9c8f00>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.seattle_data.update_many({\"address.province\" : {\"$eq\" : \"British Columbia\"}}, {\"$set\" : {\"address.province\" : \"BC\" } })\n",
    "db.seattle_data.update_many({\"address.province\" : {\"$eq\" : \"BC\"}}, {\"$set\" : {\"address.country\" : \"CA\" } })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will verify how the values for the state record. The code below shows us that several fields have street names on their state field which indicates incorrect data entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'WA - Washington', u'count': 1}\n",
      "{u'_id': u'NE 18th Street', u'count': 1}\n",
      "{u'_id': u'NE 15th Street', u'count': 1}\n",
      "{u'_id': u'W', u'count': 2}\n",
      "{u'_id': u'98107', u'count': 1}\n",
      "{u'_id': u'Washington', u'count': 26}\n",
      "{u'_id': u'wa', u'count': 53}\n",
      "{u'_id': u'Wa', u'count': 5}\n",
      "{u'_id': None, u'count': 7935530}\n",
      "{u'_id': u'156th Avenue NE', u'count': 1}\n",
      "{u'_id': u'w', u'count': 1}\n",
      "{u'_id': u'BC', u'count': 1}\n",
      "{u'_id': u'washington', u'count': 1}\n",
      "{u'_id': u'WA.', u'count': 1}\n",
      "{u'_id': u'WA', u'count': 5266}\n"
     ]
    }
   ],
   "source": [
    "group = {\"$group\":{\"_id\":\"$address.state\", \"count\":{\"$sum\":1}}}\n",
    "limit = {\"$limit\": 100}\n",
    "result = db.seattle_data.aggregate([group, limit])\n",
    "\n",
    "for r in result:\n",
    "    pprint.pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will update the records which has Washington, washington or wa the state to use the WA value. After that, we will update the country in the records located in Washington State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x57d9c8c30>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.seattle_data.update_many({\"$or\" : [{\"address.state\" : {\"$eq\" : \"washington\"}},{\"address.state\" : {\"$eq\" : \"Washington\"}}\\\n",
    "          ,{\"address.state\" : {\"$eq\" : \"wa\"}}, {\"address.state\" : {\"$eq\" : \"Wa\"}} \\\n",
    "          ,{\"address.state\" : {\"$eq\" : \"WA.\"}}, {\"address.state\" : {\"$eq\" : \"WA - Washington\"}}\\\n",
    "                                     ]}, {\"$set\" : {\"address.state\" : \"WA\" } })\n",
    "db.seattle_data.update_many({\"address.state\" : {\"$eq\" : \"WA\"}}, {\"$set\" : {\"address.country\" : \"US\" } })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that a dataset such as this has complete and standardized information. This would be useful in case we were interested in grouping the data by country, state, province or another address attribute. However, this is not true for this data.\n",
    "\n",
    "The country updates that we performed will fix the country, state and province for some of the nones. As can be verified in the query below, the number of US and CA records has increased. However, there are still some records with incorrect state and country information that we have not treated. Also, the great majority of the records do not have state, country or province information, making it more difficult to identify which country the belong to.\n",
    "\n",
    "A more efficient way to update the adress attibutes would be to use geographic coordinates of those nodes and ways. This would be better than using the approach that we used in this project as there could be records which are incorrectly labeled as being in a particular country, but with the geographic coordinates for another country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'CA', u'count': 186}\n",
      "{u'_id': u'US', u'count': 6124}\n",
      "{u'_id': None, u'count': 7934581}\n"
     ]
    }
   ],
   "source": [
    "group = {\"$group\":{\"_id\":\"$address.country\", \"count\":{\"$sum\":1}}}\n",
    "limit = {\"$limit\": 100}\n",
    "result = db.seattle_data.aggregate([group, limit])\n",
    "\n",
    "for r in result:\n",
    "    pprint.pprint(r)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
